{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "File reading:"
      ],
      "metadata": {
        "id": "eC-gZ7jIqmqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/University/Courses/CSC311/Project/manual_cleaned_data_universal.csv\""
      ],
      "metadata": {
        "id": "11rtV9ByqmbL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kNN imputation with Gower distance:"
      ],
      "metadata": {
        "id": "WcKf0rdPok87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def gower_distance(row1, row2, num_cols, cat_cols, ranges):\n",
        "    \"\"\"Calculate Gower distance between two rows\"\"\"\n",
        "    distances = []\n",
        "    valid_features = 0\n",
        "\n",
        "    # Numerical features\n",
        "    for col in num_cols:\n",
        "        if not (pd.isna(row1[col]) or pd.isna(row2[col])):\n",
        "            diff = abs(row1[col] - row2[col])\n",
        "            norm_diff = diff / ranges[col]\n",
        "            distances.append(norm_diff)\n",
        "            valid_features += 1\n",
        "\n",
        "    # Categorical features\n",
        "    for col in cat_cols:\n",
        "        if not (pd.isna(row1[col]) or pd.isna(row2[col])):\n",
        "            distances.append(0 if row1[col] == row2[col] else 1)\n",
        "            valid_features += 1\n",
        "\n",
        "    return sum(distances) / valid_features if valid_features > 0 else np.inf\n",
        "\n",
        "def knn_impute(df, num_cols, cat_cols, k=5):\n",
        "    \"\"\"Custom kNN imputation using Gower distance\"\"\"\n",
        "    # Calculate numerical ranges\n",
        "    ranges = {col: df[col].max() - df[col].min() for col in num_cols}\n",
        "\n",
        "    # Create copy to impute\n",
        "    imputed_df = df.copy()\n",
        "\n",
        "    # Find rows with missing values\n",
        "    missing_rows = imputed_df[imputed_df.isnull().any(axis=1)].index\n",
        "\n",
        "    for idx in missing_rows:\n",
        "        target_row = imputed_df.loc[idx]\n",
        "        distances = []\n",
        "\n",
        "        # Calculate distances to all complete rows\n",
        "        for other_idx, other_row in imputed_df.dropna().iterrows():\n",
        "            dist = gower_distance(target_row, other_row, num_cols, cat_cols, ranges)\n",
        "            distances.append((other_idx, dist))\n",
        "\n",
        "        # Get k-nearest neighbors\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        neighbors = imputed_df.loc[[d[0] for d in distances[:k]]]\n",
        "\n",
        "        # Impute missing values\n",
        "        for col in imputed_df.columns:\n",
        "            if pd.isna(target_row[col]):\n",
        "                if col in num_cols:\n",
        "                    # Impute mean for numerical\n",
        "                    imputed_value = neighbors[col].mean()\n",
        "                else:\n",
        "                    # Impute mode for categorical\n",
        "                    imputed_value = neighbors[col].mode()[0] if not neighbors[col].mode().empty else np.nan\n",
        "\n",
        "                imputed_df.at[idx, col] = imputed_value\n",
        "\n",
        "    return imputed_df\n"
      ],
      "metadata": {
        "id": "_bnvYMLPogOy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing:\n"
      ],
      "metadata": {
        "id": "USh9bLQxoccF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "\n",
        "\n",
        "# def process_intervals(x: str):\n",
        "#     if x.isdigit():\n",
        "#         return x\n",
        "#     interval = x.split('-')\n",
        "#     return interval[0]\n",
        "\n",
        "\n",
        "# def min_max_scaling(series: pd.Series):\n",
        "#     return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "\n",
        "def preprocess(file_path, normalize_and_onehot=False, mode=\"full\", df_in=None):\n",
        "    # If a DataFrame is provided, use it; otherwise read from file_path.\n",
        "    if df_in is not None:\n",
        "        df = df_in.copy()\n",
        "    else:\n",
        "        df = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "    # Define columns\n",
        "    num_cols = ['Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)',\n",
        "                'Q2 Cleaned', 'Q4 Cleaned']\n",
        "    cat_cols = ['Q3: In what setting would you expect this food to be served? Please check all that apply',\n",
        "                'Q5 Cleaned', 'Q6 Cleaned',\n",
        "                'Q7: When you think about this food item, who does it remind you of?',\n",
        "                'Q8: How much hot sauce would you add to this food item?',\n",
        "                'Label']\n",
        "\n",
        "    # Convert numerical cols to numeric\n",
        "    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Perform imputation\n",
        "    imputed_df = knn_impute(df, num_cols, cat_cols, k=5)\n",
        "\n",
        "    # Rename \"Q5: What movie do you think of when thinking of this food item?\",\"Q6: What drink would you pair with this food item?\"\n",
        "    # to \"Q5 Cleaned\" and \"Q6 Cleaned\"\n",
        "    df.rename(columns={\n",
        "        \"Q2: How many ingredients would you expect this food item to contain?\": \"Q2 Cleaned\",\n",
        "        \"Q4: How much would you expect to pay for one serving of this food item?\": \"Q4 Cleaned\",\n",
        "        \"Q5: What movie do you think of when thinking of this food item?\": \"Q5 Cleaned\",\n",
        "        \"Q6: What drink would you pair with this food item?\": \"Q6 Cleaned\"\n",
        "    }, inplace=True)\n",
        "    # Convert all columns to string\n",
        "    df = df.astype(str)\n",
        "\n",
        "    # Record initial row count before dropping missing values\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # Drop rows with invalid values (N/A)\n",
        "    df.replace(\"N/A\", pd.NA, inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    dropped_rows = initial_rows - len(df)\n",
        "    print(f\"Dropped {dropped_rows} rows out of {initial_rows} due to missing values.\")\n",
        "\n",
        "    # Bag-of-Words for Q5 and Q6\n",
        "    vectorizer_q5 = CountVectorizer(max_features=100)  # Limit to top 100 features\n",
        "    vectorizer_q6 = CountVectorizer(max_features=50)   # Limit to top 50 features\n",
        "\n",
        "    bow_q5 = pd.DataFrame(vectorizer_q5.fit_transform(df[\"Q5 Cleaned\"]).toarray(),\n",
        "                            columns=vectorizer_q5.get_feature_names_out())\n",
        "    bow_q6 = pd.DataFrame(vectorizer_q6.fit_transform(df[\"Q6 Cleaned\"]).toarray(),\n",
        "                            columns=vectorizer_q6.get_feature_names_out())\n",
        "    print(f\"Shape of bow_q5: {bow_q5.shape}\")\n",
        "    print(f\"Shape of bow_q6: {bow_q6.shape}\")\n",
        "\n",
        "    if mode == \"full\":\n",
        "        # Process numerical features\n",
        "        numerical_cols = [\"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\",\n",
        "                          \"Q2 Cleaned\", \"Q4 Cleaned\"]\n",
        "        for col in numerical_cols:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df.dropna(subset=numerical_cols, inplace=True)\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        normalized_numerical = pd.DataFrame(scaler.fit_transform(df[numerical_cols]), columns=numerical_cols)\n",
        "        print(f\"Shape of normalized numerical features: {normalized_numerical.shape}\")\n",
        "\n",
        "        # One-hot encode all categorical features (including Label)\n",
        "        categorical_cols = [\"Q3: In what setting would you expect this food to be served? Please check all that apply\",\n",
        "                            \"Q7: When you think about this food item, who does it remind you of?\",\n",
        "                            \"Q8: How much hot sauce would you add to this food item?\", \"Label\"]\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        encoded_categorical = pd.DataFrame(encoder.fit_transform(df[categorical_cols]),\n",
        "                                           columns=encoder.get_feature_names_out(categorical_cols))\n",
        "        print(f\"Shape of encoded categorical features: {encoded_categorical.shape}\")\n",
        "\n",
        "        final_df = pd.concat([df[\"id\"], normalized_numerical, bow_q5, bow_q6, encoded_categorical], axis=1)\n",
        "    elif mode == \"softmax\":\n",
        "        # One-hot encode only the Label column\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        encoded_label = pd.DataFrame(encoder.fit_transform(df[[\"Label\"]]),\n",
        "                                     columns=encoder.get_feature_names_out([\"Label\"]))\n",
        "        final_df = pd.concat([df[\"id\"], bow_q5, bow_q6, encoded_label], axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported mode. Use mode='full' or mode='softmax'.\")\n",
        "\n",
        "    final_df.dropna(inplace=True)  # Ensure no NaN values remain\n",
        "\n",
        "    print(f\"Preprocessed data shape: {final_df.shape}\")\n",
        "    print(f\"Preprocessed data columns: {final_df.columns}\")\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "wD1QAVOYPwg0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main:"
      ],
      "metadata": {
        "id": "ux6uw-HeuA23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWQqFiVnPa1O",
        "outputId": "97faae01-35f6-487b-e264-6fa57c8487a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 0 rows out of 1644 due to missing values.\n",
            "Shape of bow_q5: (1644, 100)\n",
            "Shape of bow_q6: (1644, 50)\n",
            "Shape of normalized numerical features: (1637, 3)\n",
            "Shape of encoded categorical features: (1637, 98)\n",
            "Preprocessed data shape: (1630, 252)\n",
            "Preprocessed data columns: Index(['id',\n",
            "       'Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)',\n",
            "       'Q2 Cleaned', 'Q4 Cleaned', 'about', 'aladdin', 'alone', 'and', 'anime',\n",
            "       'any',\n",
            "       ...\n",
            "       'Q7: When you think about this food item, who does it remind you of?_Teachers,Strangers',\n",
            "       'Q7: When you think about this food item, who does it remind you of?_nan',\n",
            "       'Q8: How much hot sauce would you add to this food item?_A little (mild)',\n",
            "       'Q8: How much hot sauce would you add to this food item?_A lot (hot)',\n",
            "       'Q8: How much hot sauce would you add to this food item?_A moderate amount (medium)',\n",
            "       'Q8: How much hot sauce would you add to this food item?_I will have some of this food item with my hot sauce',\n",
            "       'Q8: How much hot sauce would you add to this food item?_nan',\n",
            "       'Label_Pizza', 'Label_Shawarma', 'Label_Sushi'],\n",
            "      dtype='object', length=252)\n",
            "Final data matrix X shape: (1630, 250)\n",
            "Labels shape: (1630,)\n",
            "Accuracy: 0.7331288343558282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 2 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import sys\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/University/Courses/CSC311/Project/manual_cleaned_data_universal.csv\"\n",
        "def main():\n",
        "\n",
        "\n",
        "    # Use the preprocess function from utils/preprocess (bag-of-words for Q5 & Q6)\n",
        "    df = preprocess(DATASET_PATH, mode=\"full\")\n",
        "\n",
        "\n",
        "    # Identify label columns (assumed to start with \"Label\")\n",
        "    label_cols = [col for col in df.columns if col.startswith(\"Label\")]\n",
        "    # Features are all columns except \"id\" and label columns\n",
        "    feature_cols = [col for col in df.columns if col not in [\"id\"] + label_cols]\n",
        "\n",
        "    X = df[feature_cols].to_numpy()\n",
        "    y = df[label_cols].to_numpy()\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    y = np.argmax(y, axis=1)\n",
        "\n",
        "    print(f\"Final data matrix X shape: {X.shape}\")\n",
        "    print(f\"Labels shape: {y.shape}\")\n",
        "\n",
        "    # Train-test split (80% training, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "    model = QuadraticDiscriminantAnalysis()\n",
        "    model.fit(X_train, y_train)\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gower Distance for mixed data types:**\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "    \\\\\n",
        "    \\text{Numerical Features:}\\\\\n",
        "    d_{\\text{num}}(x, y) &= \\frac{|x_i - y_i|}{\\text{range}(i)} \\quad \\text{(Normalized by feature range)} \\\\\n",
        "    \\text{Categorical Features:}\\\\\n",
        "    d_{\\text{cat}}(x, y) &= \\begin{cases}\n",
        "        0 & \\text{if } x_i = y_i \\\\\n",
        "        1 & \\text{otherwise}\n",
        "    \\end{cases} \\\\\n",
        "    \\text{Total Distance:}\\\\\n",
        "    D_{\\text{Gower}} &= \\frac{\\sum d_i}{\\text{valid features count}}\n",
        "\\end{align*}\n",
        "\n"
      ],
      "metadata": {
        "id": "4yTPL244ys8m"
      }
    }
  ]
}